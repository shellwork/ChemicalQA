# ChemicalQA: Structure Reasoning and Chemical Question Answering with GRPO

This project employs the Generalized Reinforcement Policy Optimization (GRPO) method from the TRL library, optimized using the Unsloth framework to enable efficient reinforcement learning training. Unsloth significantly reduces GPU memory usage, allowing GRPO training to run effectively on a single GPU (e.g., NVIDIA RTX 4090 or A10 24G used in this project). The goal of this project is to fine-tune language models via reinforcement learning to accurately reason and answer structured chemistry-related questions, including molecular structure identification, functional group annotation, physicochemical property analysis, and answer selection.

## Key Features

- **Optimized RL Training**: Utilizes the Unsloth framework with TRL's GRPO algorithm for efficient and GPU-memory-friendly training.
- **Dataset**: Derived from MoleculeQA. The reasoning processes were generated by calling the Deepseek-R1 API using specific prompts and were subsequently curated into the final `train.jsonl` dataset.
- **Structured Reasoning Validation**: The model's output is required to adhere strictly to a structured reasoning format using `<think>` and `<answer>` tags. Format compliance is enforced through regular expression checks.
- **Semantic Embedding and Similarity Scoring**: Employs Aliyun's Qwen text embedding model (`text-embedding-v3`) for semantic similarity assessments between model outputs and reference solutions.
- **GRPO Training Workflow**: Implements LoRA (Low-Rank Adaptation) and Generalized Reward-Policy Optimization techniques for efficient reinforcement learning fine-tuning.

## Reward Functions

Two distinct reward functions are implemented to optimize the output quality:

- **Format Reward (`format_reward_func`)**:
  - Checks for strict adherence to the required structured output format (must contain `<think>` and `<answer>` tags).
  - Scores 1.0 if the format is correct, otherwise 0.0.

- **Answer Reward (`chemical_reward_func`)**:
  - Compares model outputs against the reference answer.
  - Evaluates structured reasoning (must include reasoning steps 1â€“4), semantic similarity (via text embeddings and cosine similarity), and answer accuracy.
  - Weighted scoring: semantic similarity (60%), answer accuracy (30%), format correctness (10%).

## Project Outcomes

- Established a robust reinforcement learning fine-tuning pipeline based on Unsloth and TRL.
- Designed comprehensive validation and reward mechanisms to significantly enhance the accuracy of chemical reasoning tasks.
- Provides detailed logging during training for monitoring and analysis.
- Ensures consistent high-quality structured reasoning and accurate answers.

## Quick Start

Clone the repository and install dependencies:

```bash
git clone https://github.com/shellwork/ChemicalQA
cd ChemicalQA
bash install.sh
```

- `install.sh` will install additional project dependencies suitable for typical model training cloud environments that already have the appropriate versions of Torch, CUDA, and cuDNN.

Edit the `train_ChemicalQA-grpo_unsloth.sh` file to insert your embedding model API key (e.g., from Aliyun's Qwen API), then start training:

```bash
bash train_ChemicalQA-grpo_unsloth.sh
```

Training parameters and curves can be viewed on SwanLab:

- [SwanLab Project](https://swanlab.cn/@shellwork/ChemicalQA-gpro/overview)

## Code and Dataset

- All project-related code is publicly available on GitHub: [ChemicalQA](https://github.com/shellwork/ChemicalQA)

## Logging and Outputs

- Training logs are printed in real-time and saved outputs and reasoning examples are available in the `completion_samples` directory for analysis.

For further details, refer to comments within the source code.
